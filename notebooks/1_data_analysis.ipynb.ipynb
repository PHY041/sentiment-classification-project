{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/haoyangpang/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "/Users/haoyangpang/Desktop/Y3S1/SC4002 Natural Language Processing/assignment/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/haoyangpang/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "from data_loader import preprocess_text\n",
    "from embeddings import load_pretrained_vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Rotten Tomatoes dataset\n",
    "dataset = load_dataset(\"rotten_tomatoes\")\n",
    "train_texts = dataset['train']['text']\n",
    "train_labels = dataset['train']['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 15813\n"
     ]
    }
   ],
   "source": [
    "# Preprocess all training texts to extract tokens\n",
    "all_tokens = []\n",
    "for text in train_texts:\n",
    "    tokens = preprocess_text(text)\n",
    "    all_tokens.extend(tokens)\n",
    "\n",
    "# Build vocabulary from tokens\n",
    "counter = Counter(all_tokens)\n",
    "vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "index = len(vocab)\n",
    "for token in counter:\n",
    "    vocab[token] = index\n",
    "    index += 1\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1(a): What is the size of the vocabulary formed from your training data?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: the vocabulary size for the training data is 15813"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in GloVe vocabulary: 400000\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained GloVe vocabulary\n",
    "embedding_file = '../glove.6B.300d.txt'  \n",
    "pre_trained_vocab = load_pretrained_vocab(embedding_file)\n",
    "print(f\"Number of words in GloVe vocabulary: {len(pre_trained_vocab)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of OOV words in training data: 550\n"
     ]
    }
   ],
   "source": [
    "# Identify OOV words in the training vocabulary\n",
    "training_vocab_set = set(vocab.keys())\n",
    "oov_words = training_vocab_set - pre_trained_vocab\n",
    "num_oov = len(oov_words)\n",
    "print(f\"Number of OOV words in training data: {num_oov}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1(b): How many OOV words exist in your training data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: there are 550 OOV words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1(c): Mitigating OOV Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One effective strategy to handle OOV words is to initialize their embeddings with random vectors and update them during training. This allows the model to learn representations for OOV words based on the context in which they appear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the updated version in embedding.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the vocabulary for later use\n",
    "import pickle\n",
    "\n",
    "with open('../vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(vocab, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
